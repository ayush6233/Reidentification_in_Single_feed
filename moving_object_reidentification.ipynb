{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1fcXN2j_9bQfWyTKtS21XGBcRYjc8eikJ",
      "authorship_tag": "ABX9TyP7TAjaOSbbQxzZblQR+PE9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush6233/reidentification_of_moving_object/blob/main/moving_object_reidentification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from ultralytics import YOLO\n",
        "## one sure thing   model will work better at some particular threshold , we need to. determine it\n",
        "## by running model multiple times and watching the result"
      ],
      "metadata": {
        "id": "IsObxMLHdaZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding network: ResNet50 → 128‑D\n",
        "class EmbeddingNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        backbone = models.resnet50(pretrained=True)\n",
        "        self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
        "        self.embed = nn.Linear(backbone.fc.in_features, 128)\n",
        "    def forward(self, x):\n",
        "        x = self.features(x).view(x.size(0), -1)\n",
        "        x = self.embed(x)\n",
        "        return nn.functional.normalize(x, dim=1)"
      ],
      "metadata": {
        "id": "ThqxxllyeP7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracker using 128‑D vector\n",
        "class ReIDTracker:\n",
        "    def __init__(self, sim_thresh=0.3):\n",
        "        self.next_id   = 1\n",
        "        self.templates = {}   # id -> 128‑D vector like mapping\n",
        "        self.sim_thresh= sim_thresh\n",
        "\n",
        "    def assign_ids(self, embeddings): ###gives us list of N IDs, matched greedily against existing templates\n",
        "        ##embeddings is  list of N new 128‑D vectors\n",
        "        n = len(embeddings)\n",
        "        m = len(self.templates)\n",
        "        if m == 0:\n",
        "            # no existing IDs → give every emb a new ID\n",
        "            ids = []\n",
        "            for emb in embeddings:\n",
        "                oid = self.next_id\n",
        "                self.next_id += 1\n",
        "                self.templates[oid] = emb\n",
        "                ids.append(oid)\n",
        "            return ids\n",
        "\n",
        "        # build list of (new_idx, old_id, similarity)\n",
        "        sims = []\n",
        "        old_ids = list(self.templates.keys())\n",
        "        for i, emb in enumerate(embeddings):\n",
        "            for oid in old_ids:\n",
        "                tmpl = self.templates[oid] ## used cosine similarty\n",
        "                sim = float(np.dot(emb, tmpl)/(np.linalg.norm(emb)*np.linalg.norm(tmpl)+1e-6))\n",
        "                sims.append((i, oid, sim))\n",
        "\n",
        "        # sort by similarity in descending\n",
        "        sims.sort(key=lambda x: x[2], reverse=True)\n",
        "        assigned_new = set()\n",
        "        assigned_old = set()\n",
        "        ids = [None]*n\n",
        "        # greedy match\n",
        "        for i, oid, sim in sims:\n",
        "            if sim < self.sim_thresh:\n",
        "                break\n",
        "            if i in assigned_new or oid in assigned_old:\n",
        "                continue\n",
        "            # match them\n",
        "            ids[i] = oid\n",
        "            assigned_new.add(i)\n",
        "            assigned_old.add(oid)\n",
        "            # update template\n",
        "            self.templates[oid] = 0.5*self.templates[oid] + 0.5*embeddings[i] ## important step ,\n",
        "            ## whenever we found a player that has some time appeared before then do not just update\n",
        "            ## the embedding to new one , use 50 / 50 of of both. basically take mean of them\n",
        "\n",
        "        # unmatched → new IDs\n",
        "        for i in range(n):\n",
        "            if ids[i] is None: # not matched\n",
        "                oid = self.next_id\n",
        "                self.next_id += 1\n",
        "                self.templates[oid] = embeddings[i]\n",
        "                ids[i] = oid\n",
        "        return ids"
      ],
      "metadata": {
        "id": "RCNke-aneSPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(\"/content/drive/MyDrive/best.pt\", task=\"detect\")\n",
        "print(model.names)\n",
        "print(model.info())\n",
        "# print(model.type) ## can observe how NN layers are propagated, activation functions and all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRSygAaEeWxK",
        "outputId": "a01ebd25-a760-4af4-dd67-7e53be0a58b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'ball', 1: 'goalkeeper', 2: 'player', 3: 'referee'}\n",
            "YOLOv5x summary: 285 layers, 97,203,260 parameters, 0 gradients, 246.9 GFLOPs\n",
            "(285, 97203260, 0, 246.91051520000002)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load embedding network\n",
        "device = \"cpu\" ## YOU CAN USE CUDA if you have GPU available\n",
        "embed_net = EmbeddingNet().to(device).eval()\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]), ## we should normalize, as it will be easy to decide cut off manually later\n",
        "])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49cIKkNyeb3d",
        "outputId": "d6fdb5bd-2754-42ea-d516-655b5343abbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tracker = ReIDTracker(sim_thresh=0.6) ## track re-ids"
      ],
      "metadata": {
        "id": "ex-ZQfMZed6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/15sec_input_720p.mp4\")\n",
        "## all below code is very standered to create video frame by frame and storing it in some different file\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
        "w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "h   = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out    = cv2.VideoWriter(\"tracked_output.mp4\", fourcc, fps, (w, h))"
      ],
      "metadata": {
        "id": "ZOvWa_zfeem4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgcR7Btjlu0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "480a8f56-d96d-47c9-b8ea-e18307406876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 384x640 1 ball, 16 players, 2 referees, 62.8ms\n",
            "Speed: 20.7ms preprocess, 62.8ms inference, 286.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 18 players, 2 referees, 67.4ms\n",
            "Speed: 3.3ms preprocess, 67.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ball, 16 players, 2 referees, 67.4ms\n",
            "Speed: 2.4ms preprocess, 67.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ball, 14 players, 2 referees, 67.4ms\n",
            "Speed: 2.4ms preprocess, 67.4ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ball, 14 players, 2 referees, 67.4ms\n",
            "Speed: 9.2ms preprocess, 67.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ball, 16 players, 2 referees, 67.4ms\n",
            "Speed: 2.2ms preprocess, 67.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 players, 2 referees, 67.5ms\n",
            "Speed: 2.3ms preprocess, 67.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 15 players, 1 referee, 67.4ms\n",
            "Speed: 2.2ms preprocess, 67.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 1 ball, 16 players, 1 referee, 67.4ms\n",
            "Speed: 2.4ms preprocess, 67.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "while True:\n",
        "    flag, frame = cap.read()\n",
        "    i+=1\n",
        "    if not flag or i>=10: ## break early , cpu will take long time. it is 30 fps video 15 sec , 450 frames cpu takes 4 sec to proccess 1 frame 1800 sec= 30min\n",
        "        ## for full render set i>=500 or remove i\n",
        "        break\n",
        "    # model.track(persist =True)\n",
        "    res = model(frame)[0]\n",
        "    #print(type(res)) ## object of class ultralytics engine\n",
        "    # print(res)\n",
        "    # break\n",
        "    xyxy_list= res.boxes.xyxy.tolist() ## to define a rectangle its top left and bottom right co ordinates are enough , so it contains those\n",
        "    confs    = res.boxes.conf.tolist() ## confidence intervals\n",
        "    classes  = res.boxes.cls.tolist() # only take class 2 (players)\n",
        "    bboxes, crops = [], []\n",
        "    for xyxy, conf, cls in zip(xyxy_list, confs, classes):\n",
        "        if int(cls)==2 and conf>0.3: ## class 2 means a player. and we must have above 30% confidence that it is a player\n",
        "            x1,y1,x2,y2 = map(int, xyxy)\n",
        "            bboxes.append((x1,y1,x2,y2))\n",
        "            crops.append(frame[y1:y2, x1:x2])\n",
        "    embeddings = []\n",
        "    if crops: ## if players are there\n",
        "        batch = torch.stack([preprocess(c) for c in crops]).to(device)\n",
        "        with torch.no_grad():\n",
        "            embs = embed_net(batch).cpu().numpy()\n",
        "        embeddings = [embs[i] for i in range(embs.shape[0])]\n",
        "    ids = tracker.assign_ids(embeddings) ## assign id's\n",
        "\n",
        "    for (x1,y1,x2,y2), oid in zip(bboxes, ids):\n",
        "        cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
        "        cv2.putText(frame, f\"P{oid}\", (x1,y1-10),cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
        "    out.write(frame)\n",
        "cap.release()\n",
        "out.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qtrbiZdyoNmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}